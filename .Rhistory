for (i in 1:iterations){
h <- X%*%theta
grad <-  (t(X)%*%(h - y))/m
theta <- theta - 0.0011 * grad
print(cost(X,y,theta))
print(theta)
}
return(theta)
}
iterations = 1000
theta<-gradDescent(X1,Y1,init_theta,iterations)
gradDescent<-function(X,y,theta,iterations,alpha){
m <- length(y)
grad <- rep(0,length(theta))
cost.df <- data.frame()
for (i in 1:iterations){
h <- X%*%theta
grad <-  (t(X)%*%(h - y))/m
theta <- theta - alpha * grad
cost.df <- rbind(cost.df,c(cost(X,y,theta),theta))
}
print(head(cost.df))
return(theta)
}
iterations = 100
alpha = 0.001
gradDescent(X1,Y1,init_theta,iterations,alpha)
cost.df <- data.frame(cost,theta)
cost.df
cost.df <- data.frame(cost=0,theta=0)
cost.df
gradDescent<-function(X,y,theta,iterations,alpha){
m <- length(y)
grad <- rep(0,length(theta))
cost.df <- data.frame(cost=0,theta=0)
for (i in 1:iterations){
h <- X%*%theta
grad <-  (t(X)%*%(h - y))/m
theta <- theta - alpha * grad
cost.df <- rbind(cost.df,c(cost(X,y,theta),theta))
}
print(head(cost.df))
return(theta)
}
theta<-gradDescent(X1,Y1,init_theta,iterations,alpha)
gradDescent<-function(X,y,theta,iterations,alpha){
m <- length(y)
grad <- rep(0,length(theta))
cost.df <- data.frame(cost=0,theta=0)
for (i in 1:iterations){
h <- X%*%theta
grad <-  (t(X)%*%(h - y))/m
theta <- theta - alpha * grad
cost.df <- rbind(cost.df,c(cost(X,y,theta),theta))
}
return(list(theta,cost.df))
}
theta<-gradDescent(X1,Y1,init_theta,iterations,alpha)[1]
theta
gradDescent(X1,Y1,init_theta,iterations,alpha)[2]
theta
cost.df <- data.frame(cost=NULL,theta=NULL)
cost.df
cost.df <- data.frame(cost=NA(),theta=NA)
cost.df <- data.frame(cost=NA,theta=NA)
cost.df
gradDescent<-function(X,y,theta,iterations,alpha){
m <- length(y)
grad <- rep(0,length(theta))
cost.df <- data.frame(cost=NA,theta=NA)
for (i in 1:iterations){
h <- X%*%theta
grad <-  (t(X)%*%(h - y))/m
theta <- theta - alpha * grad
cost.df <- rbind(cost.df,c(cost(X,y,theta),theta))
}
return(list(theta,cost.df))
}
theta <- gradDescent(X1,Y1,init_theta,iterations,alpha)[1]
cost.df <- gradDescent(X1,Y1,init_theta,iterations,alpha)[2]
theta
cost.df
## generic gradient descent instead
gradDescent<-function(X,y,theta,iterations,alpha){
m <- length(y)
grad <- rep(0,length(theta))
cost.df <- data.frame()
for (i in 1:iterations){
h <- X%*%theta
grad <-  (t(X)%*%(h - y))/m
theta <- theta - alpha * grad
cost.df <- rbind(cost.df,c(cost=cost(X,y,theta),theta=theta))
}
return(list(theta,cost.df))
}
theta <- gradDescent(X1,Y1,init_theta,iterations,alpha)[1]
cost.df <- gradDescent(X1,Y1,init_theta,iterations,alpha)[2]
theta
tail(cost.df)
plot(cost.df)
plot(cost.df$theta,cost.df$cost)
gradDescent<-function(X,y,theta,iterations,alpha){
m <- length(y)
grad <- rep(0,length(theta))
cost.df <- data.frame(cost=0,theta=0)
for (i in 1:iterations){
h <- X%*%theta
grad <-  (t(X)%*%(h - y))/m
theta <- theta - alpha * grad
cost.df <- rbind(cost.df,c(cost(X,y,theta),theta))
}
return(list(theta,cost.df))
}
theta <- gradDescent(X1,Y1,init_theta,iterations,alpha)[1]
cost.df <- gradDescent(X1,Y1,init_theta,iterations,alpha)[2]
plot(cost.df$theta,cost.df$cost)
cost.df
cost.df$theta
plot(cost.df[,2],cost.df[,1])
plot(cost.df[2,],cost.df[1,])
cost.df
cost.df[1]
cost.df[1][1]
cost.df[[1]]
cost.df[[1]][1]
plot(cost.df[[1]][2],cost.df[[1]][1])
cost.df[[1]][2]
cost.df[[1]][1]
plot(y=cost.df[[1]][2],x=cost.df[[1]][1])
class(cost.df[[1]][2])
as.data.frame(cost.df)
cost.df <- gradDescent(X1,Y1,init_theta,iterations,alpha)[2]
cost.df <- as.data.frame(cost.df)
ggplot(cost.df,aes(x=cost.df$theta,y=cost.df$cost)) + geom_point()
cost.df <- as.data.frame(cost.df)[-1,]
ggplot(cost.df,aes(x=cost.df$theta,y=cost.df$cost)) + geom_point()
iterations = 1000
alpha = 0.001
theta <- gradDescent(X1,Y1,init_theta,iterations,alpha)[1]
cost.df <- gradDescent(X1,Y1,init_theta,iterations,alpha)[2]
cost.df <- as.data.frame(cost.df)[-1,]
ggplot(cost.df,aes(x=cost.df$theta,y=cost.df$cost)) + geom_point()
iterations = 10000
alpha = 0.001
theta <- gradDescent(X1,Y1,init_theta,iterations,alpha)[1]
cost.df <- gradDescent(X1,Y1,init_theta,iterations,alpha)[2]
cost.df <- as.data.frame(cost.df)[-1,]
ggplot(cost.df,aes(x=cost.df$theta,y=cost.df$cost)) + geom_point()
cost(X1,Y1,theta)
theta
Y1
X1
X1<-matrix(ncol=1,nrow=nrow(df),cbind(1,df$X))
cost(X1,Y1,theta)
X1
Y1
theta
theta <- results[1][[1]]
cost(X1,Y1,theta[[1]])
library(slidify)
getwd()
setwd("C:/Users/Ilan Man/Desktop/Rpres_ML/")
slidify('index.rmd')
clus <- kmeans(as.matrix(houses),centers=3)
library(scales)  # format ggplot() axis
price <- seq(300000,600000,by=10000)
size <- price/1000 + rnorm(length(price),10,50)
houses <- data.frame(price,size)
ex <- ggplot(houses,aes(price,size))+geom_point()+scale_x_continuous(labels = comma)+
xlab("Price")+ylab("Size")+ggtitle("Square footage vs Price")
ex
clus <- kmeans(as.matrix(houses),centers=3)
center <- as.data.frame(clus$centers)
center_plot <- ex + geom_point(data=center,aes(x=center$price,y=center$size),color='blue',size=4)+
ggtitle("Groupings")
center_plot
new_p <- c(400000,300)
new_plot <- center_plot+geom_point(aes(x=400000,y=300),color='red',size=3)+ggtitle("New house")
new_plot
vec_dist <- sqrt(rowSums(t(new_p-t(houses))^2))
closest <- data.frame(houses[which.min(vec_dist),])
print(closest)
new_plot
new_plot + geom_point(data=closest,aes(x=price,y=size),color='green',size=3)+ggtitle("Closest neighbor?")
which.min(vec_dist)
new_plot + geom_point(data=closest,aes(x=price,y=size),color='green',size=3)+ggtitle("Closest neighbor?")
print(closest)
new_plot <- center_plot+geom_point(aes(x=400000,y=300),color='red',size=3)+ggtitle("New house")
price <- seq(300000,600000,by=10000)
size <- price/1000 + rnorm(length(price),10,50)
houses <- data.frame(price,size)
ex <- ggplot(houses,aes(price,size))+geom_point()+scale_x_continuous(labels = comma)+
xlab("Price")+ylab("Size")+ggtitle("Square footage vs Price")
ex
clus <- kmeans(as.matrix(houses),centers=3)
center <- as.data.frame(clus$centers)
center_plot <- ex + geom_point(data=center,aes(x=center$price,y=center$size),color='blue',size=4)+
ggtitle("Groupings")
center_plot
new_p <- c(400000,300)
new_plot <- center_plot+geom_point(aes(x=400000,y=300),color='red',size=3)+ggtitle("New house")
new_plot
vec_dist <- sqrt(rowSums(t(new_p-t(houses))^2))
closest <- data.frame(houses[which.min(vec_dist),])
print(closest)
new_plot
new_plot + geom_point(data=closest,aes(x=price,y=size),color='green',size=3)+ggtitle("Closest neighbor?")
new_house <- scale(houses)
new_new <- c((new[1]-mean(houses[,1]))/sd(houses[,1]),(new[2]-mean(houses[,2]))/sd(houses[,2]))
vec_dist <- sqrt(rowSums(t(new_new-t(new_house))^2))
which.min(vec_dist)
vec_dist <- sqrt(rowSums(t(new_new-t(new_house))^2))
new_new <- c((new[1]-mean(houses[,1]))/sd(houses[,1]),(new[2]-mean(houses[,2]))/sd(houses[,2]))
new_house <- scale(houses)
new_new <- c((new[1]-mean(houses[,1]))/sd(houses[,1]),(new[2]-mean(houses[,2]))/sd(houses[,2]))
new_p
new_new <- c((new_p[1]-mean(houses[,1]))/sd(houses[,1]),(new_p[2]-mean(houses[,2]))/sd(houses[,2]))
vec_dist <- sqrt(rowSums(t(new_new-t(new_house))^2))
which.min(vec_dist)
scale_closest <- data.frame(houses[which.min(vec_dist),])
new_plot + geom_point(data=scale_closest,aes(x=price,y=size),color='green',size=3)
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
install.packages('e1071')
library(e1071)
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
tn <- conf_mat[2,2]/sum(conf_mat[2,])
conf_mat <- matrix(c(true_positive,false_negative,false_positive,true_negative),nrow=2,ncol=2)
pred_B <- which(predict_1=="B")
predict_1 <- knn(train = train_set, test = test_set, cl = train_labels,
k = floor(sqrt(nrow(train_set))))
total_n <- nrow(scaled_data)
data <- read.table('http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', sep=',', stringsAsFactors=FALSE, header=FALSE)
library(caret)
confusionMatrix(predict_1,test_labels)$table
data <- data[,-1]
# names taken from the .names file online
n <- c("radius","texture","perimeter","area","smoothness","compactness",
"concavity","concave_points","symmetry","fractal")
ind <- c("mean","std","worst")
headers<-as.character()
for(i in ind){
headers<-c(headers,paste(n,i))
}
names(data)<-c("diagnosis",headers)
str(data[,1:10])
scaled_data <- as.data.frame(lapply(data[,-1], scale))
scaled_data <- cbind(diagnosis=data$diagnosis, scaled_data)
head(scaled_data[2:6])
pred_B <- which(predict_1=="B")
actual_B <- which(scaled_data[,1]=="B")
pred_M <- which(predict_1=="M")
predict_1 <- knn(train = scaled_data[,2:31], test = scaled_data[,2:31],
cl = scaled_data[,1],
k = floor(sqrt(nrow(scaled_data))))
table(predict_1)
pred_B <- which(predict_1=="B")
actual_B <- which(scaled_data[,1]=="B")
pred_M <- which(predict_1=="M")
actual_M <- which(scaled_data[,1]=="M")
true_positive <- sum(pred_B %in% actual_B)
true_negative <- sum(pred_M %in% actual_M)
false_positive <- sum(pred_B %in% actual_M)
false_negative <- sum(pred_M %in% actual_B)
conf_mat <- matrix(c(true_positive,false_positive,false_negative,true_negative),nrow=2,ncol=2)
conf_mat
acc <- sum(diag(conf_mat))/sum(conf_mat)
tpr <- conf_mat[1,1]/sum(conf_mat[1,])
tn <- conf_mat[2,2]/sum(conf_mat[2,])
false_negative
pred_M %in% actual_B
pred_M
actual_B
which(pred_M %in% actual_B)
which(pred_B %in% actual_M)
c(acc=acc,tpr=tpr,tn=tn)
conf_mat
colnames(conf_mat) <- c('Actual B','Actual M')
rownames(conf_mat) <- c('Pred B','Pred M')
conf_mat
total_n <- nrow(scaled_data)
train_ind <- sample(total_n,total_n*2/3)
train_labels <- scaled_data[train_ind,1]
test_labels <- scaled_data[-train_ind,1]
train_set <- scaled_data[train_ind,2:31]
test_set <- scaled_data[-train_ind,2:31]
predict_1 <- knn(train = train_set, test = test_set, cl = train_labels,
k = floor(sqrt(nrow(train_set))))
table(predict_1)
pred_B <- which(predict_1=="B")
test_B <- which(test_labels=="B")
test_M <- which(test_labels=="M")
pred_M <- which(predict_1=="M")
true_positive <- sum(pred_B %in% test_B)
true_negative <- sum(pred_M %in% test_M)
false_positive <- sum(pred_B %in% test_M)
false_negative <- sum(pred_M %in% test_B)
conf_mat <- matrix(c(true_positive,false_negative,false_positive,true_negative),nrow=2,ncol=2)
acc <- sum(diag(conf_mat))/sum(conf_mat)
tpr <- conf_mat[1,1]/sum(conf_mat[1,])
tn <- conf_mat[2,2]/sum(conf_mat[2,])
c(acc=acc,tpr=tpr,tn=tn)
conf_mat
colnames(conf_mat) <- c('Actual B','Actual M')
rownames(conf_mat) <- c('Pred B','Pred M')
conf_mat
library(caret)
confusionMatrix(predict_1,test_labels)$table
con_mat <- confusionMatrix(predict_1,test_labels)
con_mat
str(con_mat)
con_mat$
summary
conf_mat$overall
con_mat$overall
con_mat$byClass
con_mat$table
con_mat$table[1]
diag(con_mat$table)
sum(diag(con_mat$table))/sum(con_mat$table)
con_mat[1]/sum(con_mat[,1])
con_mat[1]/sum(con_mat$table[,1])
con_mat$table[,1]
sum(con_mat$table[,1])
con_mat[1]/sum(con_mat$table[,12])
con_mat[1]/sum(con_mat$table[,2])
con_mat$table[1]/sum(con_mat$table[,2])
con_mat$table[1]/sum(con_mat$table[2,])
con_mat$table[2,]
con_mat$table[1]/sum(con_mat$table[1,])
library(xtable)
performances <- data.frame(per)
k_params <- c(1,3,5,10,15,20,25,30,40)
perf_acc <- NULL
per <- NULL
for(i in k_params){
predictions <- knn(train = train_set,
test = test_set,
cl = train_labels,
k = i)
conf <- confusionMatrix(predictions,test_labels)$table
perf_acc <- sum(diag(conf))/sum(conf)
per <- rbind(per,c(i, perf_acc,conf[[1]],conf[[3]],conf[[2]],conf[[4]]))
}
performances <- data.frame(per)
names(performances) <- c("K","Acc","TP","FP","FN","TN")
tab <- xtable(performances)
print(tab, type='html')
print(tab, type='html',include.rownames=FALSE)
slidify('index.rmd')
scale_closest <- data.frame(houses[which.min(vec_dist),])
scale_closest
hist(data$diagnosis)
data$diagnosis
hist(as.numeric(data$diagnosis))
as.numeric(data$diagnosis)
prop.table(table(data$diagnosis))
head(data)[2:6]
prop.table(predict_1)
prop.table(as.numeric(as.character(predict_1)))
prop.table(as.numeric(as.character(predict_1)))
predict_1
as.character(predict_1)
prop.table(as.character(predict_1))
prop.table(table(predict_1))
predict_1 <- knn(train = scaled_data[,2:31],
test = scaled_data[,2:31],
cl = scaled_data[,1],
k = floor(sqrt(nrow(scaled_data))))
prop.table(table(predict_1))
predict_1 <- knn(train = train_set, test = test_set, cl = train_labels,
k = floor(sqrt(nrow(train_set))))
prop.table(table(predict_1))
?knn
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
install.packages('tm')
slidify('index.rmd')
?tm_map
stopwords()
?stopwords()
pred <- prediction(predictions = as.numeric(comparison$predict), labels= raw_test_set[,1])
confidence <- predict(naive_model, corpus_test_set, type='raw')
?createDataPartition
df
x <- seq(0,1,by=0.01)
df <- data.frame(cbind(X=x,Y=y))
y <- 4 + 3*x*rnorm(length(x),2,sd=0.3)
y <- 4 + 3*x*rnorm(length(x),2,sd=0.3)
df <- data.frame(cbind(X=x,Y=y))
x <- cbind(1,x)  #Add ones to x
theta<- c(0,0)  # initalize theta vector
m <- nrow(x)  # Number of the observations
grad_cost <- function(X,y,theta) return(sum(((X%*%theta)- y)^2))
gradDescent<-function(X,y,theta,iterations,alpha){
m <- length(y)
grad <- rep(0,length(theta))
cost.df <- data.frame(cost=0,theta=0)
for (i in 1:iterations){
h <- X%*%theta  # Linear hypothesis function
grad <-  (t(X)%*%(h - y))/m
theta <- theta - alpha * grad # Update theta
cost.df <- rbind(cost.df,c(grad_cost(X,y,theta),theta))
}
return(list(theta,cost.df))
}
X1<-matrix(ncol=1,nrow=nrow(df),cbind(1,df$X))
Y1<-matrix(ncol=1,nrow=nrow(df),df$Y)
init_theta<-as.matrix(c(0))
grad_cost(X1,Y1,init_theta)
iterations = 10000
alpha = 0.1
results <- gradDescent(X1,Y1,init_theta,iterations,alpha)
theta <- results[1][[1]]
cost.df <- results[2]
cost.df <- as.data.frame(cost.df)[-1,]
ggplot(cost.df,aes(x=cost.df$theta,y=cost.df$cost)) + geom_point()+
ylab("Cost")+xlab("Theta")+ggtitle("Cost curve")
grad_cost(X1,Y1,theta[[1]])
intercept <- df[df$X==0,]$Y
pred <- function (x) return(intercept+c(x)%*%theta)
pred <- function (x) return(intercept+x%*%theta)
new_points <- c(0.1,0.5,0.8,1.1)
new_preds <- data.frame(X=new_points,Y=sapply(new_points,pred))
```{r new_point,echo=TRUE,fig.height=5,fig.width=5}
ggplot(data=df,aes(x=X,y=Y))+geom_point(size=2)
ggplot(data=df,aes(x=X,y=Y))+geom_point()+geom_point(data=new_preds,aes(x=X,y=Y,color='red'),size=3)+scale_colour_discrete(guide = FALSE)
df <- transform(df, X4=X^4, X5=X^5, X6=X^6, X7=X^7, X8=X^8, X9=X^9, X10=X^10, X11=X^11,X12=X^12, X13=X^13, X14=X^14,X15=X^15)
line.fit <- lm(Y~X+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15,df)
summary(line.fit)$coef[,1]
df <- transform(df, X2=X^2, X3=X^3)
df <- transform(df, X4=X^4, X5=X^5, X6=X^6, X7=X^7, X8=X^8, X9=X^9, X10=X^10, X11=X^11,X12=X^12, X13=X^13, X14=X^14,X15=X^15)
line.fit <- lm(Y~X+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15,df)
summary(line.fit)$coef[,1]
sqrt(mean((predict(line.fit)-df$Y)^2))
sqrt(mean((predict(line.fit)-df$Y)^2))
summary(line.fit)$coef[,1]
df <- transform(df, X4=X^4, X5=X^5, X6=X^6, X7=X^7, X8=X^8, X9=X^9, X10=X^10, X11=X^11,X12=X^12, X13=X^13, X14=X^14,X15=X^15,X16=X^16,X17=X^17,X18=X^18,X19=X^19,X20=X^20)
line.fit <- lm(Y~X+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20,df)
summary(line.fit)$coef[,1]
sqrt(mean((predict(line.fit)-df$Y)^2))  # Root MSE
ortho.coefs <- with(df,cor(poly(X,degree=3)))
sum(ortho.coefs[upper.tri(ortho.coefs)]) # polynomials are uncorrelated
linear.fit <- lm(Y~poly(X,degree=20),df)
summary(linear.fit)$coef[,1]
summary(linear.fit)$adj.r.squared # R^2 is 98% and no errors
sqrt(mean((predict(linear.fit)-df$Y)^2)) # RMSE = 0.472
x <- seq(0,1,by=0.005)
y <- sin(3*pi*x) + rnorm(length(x),0,0.1)
indices <- sort(sample(1:length(x), round(0.5 * length(x))))
indices <- sort(sample(1:length(x), round(0.5 * length(x))))
indices
indices <- sort(sample(length(x), round(0.5 * length(x))))
indices
x <- seq(0,1,by=0.005)
y <- sin(3*pi*x) + rnorm(length(x),0,0.1)
indices <- sort(sample(length(x), round(0.5 * length(x))))
training.x <- x[indices]
training.y <- y[indices]
test.x <- x[-indices]
test.y <- y[-indices]
training.df <- data.frame(X = training.x, Y = training.y)
test.df <- data.frame(X = test.x, Y = test.y)
rmse <- function(y,h) return(sqrt(mean((y-h)^2)))
performance <- data.frame()
degrees <- 1:20
for (d in degrees){
fits <- lm(Y~poly(X,degree=d),data=training.df)
performance <- rbind(performance, data.frame(Degree = d,
Data = 'Training',
RMSE = rmse(training.y, predict(fits))))
performance <- rbind(performance, data.frame(Degree = d,
Data = 'Test',
RMSE = rmse(test.y, predict(fits,
newdata = test.df))))
}
ggplot(performance, aes(x = Degree, y = RMSE, linetype = Data)) + geom_point() + geom_line()
slidify('index.rmd')
slidify('index.rmd')
auc
sqrt(mean((predict(line.fit)-df$Y)^2))  # Root MSE
summary(line.fit)$coef[,1]
summary(line.fit)$coef[,1][1:5]
head(summary(line.fit)$coef[,1])
sqrt(mean((predict(line.fit)-df$Y)^2))  # Root MSE
sqrt(mean((predict(linear.fit)-df$Y)^2)) # RMSE = 0.472
slidify('index.rmd')
